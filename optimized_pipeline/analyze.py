#!/usr/bin/env python3
"""
GEMBA ê°œì„  íš¨ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ë³€ê²½ í›„ ê²°ê³¼ ë¶„ì„
"""

import json
import statistics
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd

class GembaAnalyzer:
    def __init__(self, result_dir: Path):
        self.result_dir = result_dir
        self.data = None
        
    def load_results(self) -> bool:
        """ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            # GEMBA ê²°ê³¼ íŒŒì¼ ë¡œë“œ
            gemba_file = self.result_dir / "gemba.json"
            if not gemba_file.exists():
                print(f"âŒ GEMBA ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {gemba_file}")
                return False
            
            with open(gemba_file, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            
            print(f"âœ… {len(self.data)}ê°œ ë ˆì½”ë“œ ë¡œë“œë¨")
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def analyze_gemba_scores(self) -> Dict[str, Any]:
        """GEMBA ì ìˆ˜ ë¶„ì„"""
        if not self.data:
            return {}
        
        # ê¸°ë³¸ í†µê³„
        overall_scores = [item['gemba'] for item in self.data]
        adequacy_scores = [item['gemba_adequacy'] for item in self.data]
        fluency_scores = [item['gemba_fluency'] for item in self.data]
        
        # ê¸¸ì´ë³„ ë¶„ì„
        bucket_analysis = {}
        for bucket in ['very_short', 'short', 'medium', 'long', 'very_long']:
            bucket_items = [item for item in self.data if item['bucket'] == bucket]
            if bucket_items:
                bucket_scores = [item['gemba'] for item in bucket_items]
                bucket_analysis[bucket] = {
                    'count': len(bucket_items),
                    'avg_score': statistics.mean(bucket_scores),
                    'pass_rate': len([s for s in bucket_scores if s >= 75]) / len(bucket_scores) * 100
                }
        
        return {
            'total_samples': len(self.data),
            'overall_stats': {
                'avg_overall': statistics.mean(overall_scores),
                'avg_adequacy': statistics.mean(adequacy_scores),
                'avg_fluency': statistics.mean(fluency_scores),
                'median_overall': statistics.median(overall_scores),
                'std_overall': statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0
            },
            'score_distribution': {
                'excellent_90+': len([s for s in overall_scores if s >= 90]),
                'good_80_89': len([s for s in overall_scores if 80 <= s < 90]),
                'acceptable_70_79': len([s for s in overall_scores if 70 <= s < 80]),
                'poor_60_69': len([s for s in overall_scores if 60 <= s < 70]),
                'failing_below_60': len([s for s in overall_scores if s < 60])
            },
            'bucket_analysis': bucket_analysis
        }
    
    def analyze_quality_decisions(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²°ì • ë¶„ì„"""
        if not self.data:
            return {}
        
        # íƒœê·¸ ë¶„í¬
        tag_counts = {}
        for item in self.data:
            tag = item.get('tag', 'unknown')
            tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        # ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ ë¶„ì„
        failed_items = [item for item in self.data if item.get('tag') == 'fail']
        fail_reasons = {}
        
        for item in failed_items:
            failed_checks = item.get('flag', {}).get('failed', [])
            for reason in failed_checks:
                fail_reasons[reason] = fail_reasons.get(reason, 0) + 1
        
        return {
            'tag_distribution': tag_counts,
            'pass_rate': (tag_counts.get('strict_pass', 0) + tag_counts.get('soft_pass', 0)) / len(self.data) * 100,
            'strict_pass_rate': tag_counts.get('strict_pass', 0) / len(self.data) * 100,
            'soft_pass_rate': tag_counts.get('soft_pass', 0) / len(self.data) * 100,
            'fail_rate': tag_counts.get('fail', 0) / len(self.data) * 100,
            'failure_reasons': fail_reasons
        }
    
    def analyze_evidence_quality(self) -> Dict[str, Any]:
        """Evidence í’ˆì§ˆ ë¶„ì„"""
        if not self.data:
            return {}
        
        evidence_lengths = []
        evidence_types = {
            'perfect_excellent': 0,
            'specific_issues': 0,
            'generic_feedback': 0,
            'empty_or_minimal': 0
        }
        
        for item in self.data:
            evidence = item.get('flag', {}).get('gemba_reason', '')
            evidence_lengths.append(len(evidence))
            
            # Evidence ìœ í˜• ë¶„ë¥˜
            evidence_lower = evidence.lower()
            if any(word in evidence_lower for word in ['perfect', 'excellent', 'flawless']):
                evidence_types['perfect_excellent'] += 1
            elif len(evidence) > 50 and any(word in evidence_lower for word in 
                                          ['grammar', 'terminology', 'accuracy', 'fluency']):
                evidence_types['specific_issues'] += 1
            elif len(evidence) > 20:
                evidence_types['generic_feedback'] += 1
            else:
                evidence_types['empty_or_minimal'] += 1
        
        return {
            'avg_evidence_length': statistics.mean(evidence_lengths) if evidence_lengths else 0,
            'evidence_types': evidence_types,
            'detailed_feedback_rate': (evidence_types['specific_issues'] / len(self.data) * 100) if self.data else 0
        }
    
    def compare_with_baseline(self, baseline_stats: Dict = None) -> Dict[str, Any]:
        """ê¸°ì¤€ì„ ê³¼ ë¹„êµ (ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ê±°ë‚˜ ì´ì „ ê²°ê³¼ì™€ ë¹„êµ)"""
        
        # ì¼ë°˜ì ì¸ GPT-3.5-turbo ê¸°ì¤€ì„  (ì¶”ì •ê°’)
        if baseline_stats is None:
            baseline_stats = {
                'avg_overall': 72.5,
                'pass_rate': 65.0,
                'detailed_feedback_rate': 40.0,
                'avg_evidence_length': 35
            }
        
        current_stats = self.analyze_gemba_scores()
        quality_stats = self.analyze_quality_decisions()
        evidence_stats = self.analyze_evidence_quality()
        
        improvements = {
            'score_improvement': current_stats['overall_stats']['avg_overall'] - baseline_stats['avg_overall'],
            'pass_rate_improvement': quality_stats['pass_rate'] - baseline_stats['pass_rate'],
            'evidence_quality_improvement': evidence_stats['detailed_feedback_rate'] - baseline_stats['detailed_feedback_rate'],
            'evidence_length_improvement': evidence_stats['avg_evidence_length'] - baseline_stats['avg_evidence_length']
        }
        
        return {
            'baseline': baseline_stats,
            'current': {
                'avg_overall': current_stats['overall_stats']['avg_overall'],
                'pass_rate': quality_stats['pass_rate'],
                'detailed_feedback_rate': evidence_stats['detailed_feedback_rate'],
                'avg_evidence_length': evidence_stats['avg_evidence_length']
            },
            'improvements': improvements
        }
    
    def print_analysis_report(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("ğŸ¯ GEMBA ê°œì„  íš¨ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        if not self.load_results():
            return
        
        # 1. GEMBA ì ìˆ˜ ë¶„ì„
        gemba_stats = self.analyze_gemba_scores()
        print(f"\nğŸ“Š GEMBA ì ìˆ˜ ë¶„ì„ (ì´ {gemba_stats['total_samples']}ê°œ ìƒ˜í”Œ)")
        print("-" * 40)
        overall = gemba_stats['overall_stats']
        print(f"í‰ê·  Overall ì ìˆ˜: {overall['avg_overall']:.1f}")
        print(f"í‰ê·  Adequacy: {overall['avg_adequacy']:.1f}")
        print(f"í‰ê·  Fluency: {overall['avg_fluency']:.1f}")
        print(f"ì¤‘ì•™ê°’: {overall['median_overall']:.1f}")
        print(f"í‘œì¤€í¸ì°¨: {overall['std_overall']:.1f}")
        
        # ì ìˆ˜ ë¶„í¬
        print(f"\nì ìˆ˜ ë¶„í¬:")
        dist = gemba_stats['score_distribution']
        print(f"  ğŸŒŸ ìš°ìˆ˜ (90+): {dist['excellent_90+']}ê°œ ({dist['excellent_90+']/gemba_stats['total_samples']*100:.1f}%)")
        print(f"  âœ… ì–‘í˜¸ (80-89): {dist['good_80_89']}ê°œ ({dist['good_80_89']/gemba_stats['total_samples']*100:.1f}%)")
        print(f"  âš ï¸  ë³´í†µ (70-79): {dist['acceptable_70_79']}ê°œ ({dist['acceptable_70_79']/gemba_stats['total_samples']*100:.1f}%)")
        print(f"  ğŸ”´ ë¯¸í¡ (60-69): {dist['poor_60_69']}ê°œ ({dist['poor_60_69']/gemba_stats['total_samples']*100:.1f}%)")
        print(f"  âŒ ì‹¤íŒ¨ (<60): {dist['failing_below_60']}ê°œ ({dist['failing_below_60']/gemba_stats['total_samples']*100:.1f}%)")
        
        # ê¸¸ì´ë³„ ë¶„ì„
        print(f"\nğŸ“ ê¸¸ì´ë³„ ì„±ëŠ¥:")
        bucket_stats = gemba_stats['bucket_analysis']
        for bucket, stats in bucket_stats.items():
            print(f"  {bucket:12}: {stats['count']:3}ê°œ, í‰ê·  {stats['avg_score']:5.1f}, í†µê³¼ìœ¨ {stats['pass_rate']:5.1f}%")
        
        # 2. í’ˆì§ˆ ê²°ì • ë¶„ì„
        quality_stats = self.analyze_quality_decisions()
        print(f"\nğŸ¯ í’ˆì§ˆ ê²°ì • ë¶„ì„")
        print("-" * 40)
        print(f"ì „ì²´ í†µê³¼ìœ¨: {quality_stats['pass_rate']:.1f}%")
        print(f"Strict Pass: {quality_stats['strict_pass_rate']:.1f}%")
        print(f"Soft Pass: {quality_stats['soft_pass_rate']:.1f}%")
        print(f"ì‹¤íŒ¨ìœ¨: {quality_stats['fail_rate']:.1f}%")
        
        if quality_stats['failure_reasons']:
            print(f"\nì‹¤íŒ¨ ì›ì¸:")
            for reason, count in quality_stats['failure_reasons'].items():
                print(f"  {reason}: {count}ê°œ")
        
        # 3. Evidence í’ˆì§ˆ ë¶„ì„
        evidence_stats = self.analyze_evidence_quality()
        print(f"\nğŸ’¬ Evidence í’ˆì§ˆ ë¶„ì„")
        print("-" * 40)
        print(f"í‰ê·  Evidence ê¸¸ì´: {evidence_stats['avg_evidence_length']:.1f} ê¸€ì")
        print(f"êµ¬ì²´ì  í”¼ë“œë°± ë¹„ìœ¨: {evidence_stats['detailed_feedback_rate']:.1f}%")
        
        ev_types = evidence_stats['evidence_types']
        total = sum(ev_types.values())
        print(f"\nEvidence ìœ í˜•ë³„ ë¶„í¬:")
        print(f"  ì™„ë²½/ìš°ìˆ˜: {ev_types['perfect_excellent']}ê°œ ({ev_types['perfect_excellent']/total*100:.1f}%)")
        print(f"  êµ¬ì²´ì  ì´ìŠˆ: {ev_types['specific_issues']}ê°œ ({ev_types['specific_issues']/total*100:.1f}%)")
        print(f"  ì¼ë°˜ì  í”¼ë“œë°±: {ev_types['generic_feedback']}ê°œ ({ev_types['generic_feedback']/total*100:.1f}%)")
        print(f"  ë¹ˆì•½/ìµœì†Œ: {ev_types['empty_or_minimal']}ê°œ ({ev_types['empty_or_minimal']/total*100:.1f}%)")
        
        # 4. ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„ ë„
        comparison = self.compare_with_baseline()
        print(f"\nğŸš€ ê°œì„  íš¨ê³¼ (GPT-3.5-turbo ëŒ€ë¹„)")
        print("-" * 40)
        improvements = comparison['improvements']
        print(f"í‰ê·  ì ìˆ˜ ê°œì„ : {improvements['score_improvement']:+.1f}ì ")
        print(f"í†µê³¼ìœ¨ ê°œì„ : {improvements['pass_rate_improvement']:+.1f}%")
        print(f"êµ¬ì²´ì  í”¼ë“œë°± ê°œì„ : {improvements['evidence_quality_improvement']:+.1f}%")
        print(f"Evidence ê¸¸ì´ ê°œì„ : {improvements['evidence_length_improvement']:+.1f} ê¸€ì")
        
        # 5. ì¢…í•© í‰ê°€
        print(f"\nğŸ“‹ ì¢…í•© í‰ê°€")
        print("=" * 40)
        score_improvement = improvements['score_improvement']
        if score_improvement > 5:
            print("ğŸ‰ ìš°ìˆ˜: ìƒë‹¹í•œ ì ìˆ˜ ê°œì„  ë‹¬ì„±!")
        elif score_improvement > 2:
            print("âœ… ì–‘í˜¸: ì˜ë¯¸ìˆëŠ” ê°œì„  í™•ì¸")
        elif score_improvement > 0:
            print("âš ï¸  ë³´í†µ: ì•½ê°„ì˜ ê°œì„  ìˆìŒ")
        else:
            print("ğŸ”´ ë¯¸í¡: ê°œì„  í•„ìš”")
        
        # ì¶”ì²œ ì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œ ì‚¬í•­:")
        if evidence_stats['detailed_feedback_rate'] < 60:
            print("- Evidence í’ˆì§ˆ ë” í–¥ìƒ í•„ìš” (ë” êµ¬ì²´ì ì¸ í”¼ë“œë°±)")
        if quality_stats['fail_rate'] > 30:
            print("- ì‹¤íŒ¨ìœ¨ì´ ë†’ìŒ: ì„ê³„ê°’ ì¡°ì • ê²€í†  í•„ìš”")
        if gemba_stats['overall_stats']['std_overall'] > 15:
            print("- ì ìˆ˜ í¸ì°¨ê°€ í¼: ë” ì¼ê´€ëœ í‰ê°€ í•„ìš”")
        
        print(f"\në¶„ì„ ì™„ë£Œ! ğŸ¯")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GEMBA ê°œì„  íš¨ê³¼ ë¶„ì„")
    parser.add_argument("--version", "-v", default="v10", help="ë¶„ì„í•  ë²„ì „ (ê¸°ë³¸: v10)")
    parser.add_argument("--output", "-o", help="ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•  ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì„¤ì • - optimized_pipelineì˜ out ë””ë ‰í† ë¦¬ í™•ì¸
    base_dir = Path(__file__).parent
    result_dir = base_dir / "out" / args.version
    
    # optimized_pipelineë„ í™•ì¸
    optimized_result_dir = base_dir.parent / "optimized_pipeline" / "out" / args.version
    
    if result_dir.exists():
        target_dir = result_dir
    elif optimized_result_dir.exists():
        target_dir = optimized_result_dir
        print(f"ğŸ“ optimized_pipelineì˜ ê²°ê³¼ ì‚¬ìš©: {target_dir}")
    else:
        print(f"âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:")
        print(f"   {result_dir}")
        print(f"   {optimized_result_dir}")
        return
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = GembaAnalyzer(target_dir)
    analyzer.print_analysis_report()
    
    # JSON ì¶œë ¥ (ì„ íƒì‚¬í•­)
    if args.output:
        if analyzer.load_results():
            analysis_data = {
                'gemba_scores': analyzer.analyze_gemba_scores(),
                'quality_decisions': analyzer.analyze_quality_decisions(),
                'evidence_quality': analyzer.analyze_evidence_quality(),
                'comparison': analyzer.compare_with_baseline()
            }
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
